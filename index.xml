<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>金谷风的博客</title>
    <link>https://gufengjin770.github.io/</link>
    <description>Recent content on 金谷风的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Mar 2024 21:20:34 +0000</lastBuildDate>
    <atom:link href="https://gufengjin770.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kmeans</title>
      <link>https://gufengjin770.github.io/post/kmeans/</link>
      <pubDate>Wed, 06 Mar 2024 21:20:34 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/kmeans/</guid>
      <description>kmeans++算法 K均值++（K-means++）是一种改进的K均值聚类算法，旨在解决传统K均值算法在初始质心选择上的随机性问题，提高了算法的收敛速度和聚类质量。&#xA;K均值算法是一种常用的无监督聚类方法，其基本思想是将数据集划分为K个簇，使得同一簇内的数据点彼此相似，而不同簇之间的数据点尽可能不相似。该算法的主要步骤包括选择初始质心、分配数据点到最近的质心所属的簇、更新质心位置等。&#xA;K均值++算法通过改进初始质心的选择，来提高K均值算法的效果。其主要思想是在选择初始质心时，首先选择一个质心作为第一个簇的中心，然后按照一定的概率分布选择下一个质心，使得离已选择的质心越远的点被选作下一个质心的概率越大，以此类推，直到选择完所有的初始质心。这样做可以避免初始质心选择过于集中或者过于分散的情况，提高了算法的鲁棒性和收敛速度。&#xA;K均值++算法的步骤如下：&#xA;从数据集中随机选择一个点作为第一个质心。 对于数据集中的每个点，计算其到已选择的质心的距离，并选择一个距离当前已选择质心最远的点作为下一个质心，选择的概率与距离的平方成正比。 重复步骤2，直到选择完所有的质心。 使用K均值算法进行迭代优化，直到质心不再发生变化或者达到最大迭代次数。 K均值++算法相比传统K均值算法，在相同迭代次数下能够得到更好的聚类效果，并且对初始质心的选择更加稳定和可靠。因此，在实际应用中，K均值++算法更为常用。&#xA;import numpy as np # 使用库函数计算欧氏距离 def euclidean(point1, point2): return np.linalg.norm(point1 - point2) # 使用K均值++算法选择初始质心 def initialize_centroids(data, num_clusters): centroids = np.zeros((num_clusters, data.shape[1])) centroids[0] = data[np.random.randint(0, len(data))] for i in range(1, num_clusters): distances = np.array([min([euclidean(point, centroid) for centroid in centroids[:i]]) for point in data]) probabilities = distances / distances.sum() centroids[i] = data[np.random.choice(len(data), p=probabilities)] return centroids # K均值算法 def kmeans(data, num_clusters, max_iter=100, tol=1e-4): centroids = initialize_centroids(data, num_clusters) for _ in range(max_iter): # 分配数据点到最近的质心 clusters = {i: [] for i in range(num_clusters)} for point in data: distances = [euclidean(point, centroid) for centroid in centroids] cluster = np.</description>
    </item>
    <item>
      <title>不依靠PyTorch和TensorFlow做手写数字识别</title>
      <link>https://gufengjin770.github.io/post/deeplearning2/</link>
      <pubDate>Mon, 04 Mar 2024 04:53:54 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/deeplearning2/</guid>
      <description>不依靠PyTorch和TensorFlow做手写数字识别 这章我们来看下如何不通过调用python深度学习的库来写CNN网络。&#xA;所有机器学习都可以看成三个步骤：&#xA;定义方程的框架，输入是什么 输出是什么 中间如何从输入到输出&#xA;定义损失函数 衡量模型预测与实际目标之间差异的指标。&#xA;最优化算法 根据定义的框架和损失函数找出模型最优的参数&#xA;在下列例子中我们用到的是MNIST 数据集，一个包含手写数字图像的经典数据集，用于机器学习和模式识别的基准测试。这些图像是 28x28 像素的灰度图像，表示手写的数字 0 到 9。在代码中，x_train 包含了 60000 个训练样本，每个样本是一个经过归一化处理的大小为 784 (28x28) 的一维数组，代表一个手写数字图像。而 t_train 则是每个样本对应的标签，使用了 one-hot 编码，表示了对应的数字类别。&#xA;下载数据集&#xA;import sys, os sys.path.append(os.pardir) import numpy as np from dataset.mnist import load_mnist from collections import OrderedDict (x_train, t_train), (x_test, t_test) = \ load_mnist(normalize=True, one_hot_label=True) print(x_train.shape) # (60000, 784) print(t_train.shape) # (60000, 10) 打印下图片&#xA;import matplotlib.pyplot as plt # 打印前两个样本及其标签 num_samples_to_display = 2 for i in range(num_samples_to_display): sample_image = x_train[i].</description>
    </item>
    <item>
      <title>初识神经网络：如何用梯度下降法训练一个简单的线性模型</title>
      <link>https://gufengjin770.github.io/post/deeplearning1/</link>
      <pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/deeplearning1/</guid>
      <description>初识神经网络：如何用梯度下降法训练一个简单的线性模型 数学模型是什么？ 数学模型能够从数据中学习并进行预测或决策。这些模型可以用于解决各种问题，包括分类 Classification, 回归 Regression等。模型通常被用来发现数据中的模式和关联，从而进行预测或做出决策。&#xA;模型通常包含输入值和输出值。输入值 Input通常是一组特征或属性，用来描述数据的各个方面。输出值 Output则是模型根据输入值进行预测或分类的结果。&#xA;在监督学习 Supervised learning中，模型通过学习输入值和对应的输出值之间的关系来进行预测或分类。这些输入值和输出值通常被组织成训练集 Training set和验证集 Validation set，模型通过训练集学习输入和输出之间的映射关系。然后使用验证集来评估模型的性能，以便更好地了解模型在未见过的数据上的表现。通过与训练集分开的独立数据进行评估，可以更客观地判断模型的泛化能力，即模型对未知数据的适应能力。一旦训练完成，模型就可以用来对新的输入值进行预测或分类。&#xA;在 无监督学习 Unsupervised learning中，模型不需要标记的输出值，而是试图发现数据中的结构或模式。输入值被用来描述数据，而模型则尝试根据这些输入值发现数据的内在结构或者进行聚类等任务。&#xA;如何用梯度下降法 Gradient Descent Method训练一个简单的线性模型？ 先来看下如何训练监督学习类问题，我们将用梯度下降法训练一个简单的线性模型。&#xA;首先，我们需要准备包含输入值和对应输出值（标签）的数据集。在下面代码中例子里输入值是 $x = [1, 2, 3, 4]$，输出值是 $y = [2, 3, 4, 5]$。&#xA;第二步选择模型，在这个例子中我们选择线性模型， $y_{pred} = ax + b$, 其中x是输入值，$y_{pred}$是预测值，而a和b是模型的两个参数。&#xA;第三步定义损失函数，我们的目标是模型能根据输入的$x$准确的输出$y$的值，所以我们需要预测值尽可能的等于输出值，也就是$y-y_{pred}$ 尽可能接近零。我们的损失函数是均方误差（MSE），定义如下： $$ \frac{1}{n}\sum_{i=1}^{n}\left ( y_{i}-(ax_{i}+b) \right )^{2} $$ 其中，$n$ 是样本数量，$y_i$ 是实际标签，$x_i$ 是对应的特征值。我们的目标就是通过优化算法修改参数 (a,b) 让损失函数尽可能小。&#xA;第四部选择优化算法，我们选择梯度下降法训练。梯度下降法包含四个步骤，&#xA;前向传播（Forward Propagation）：使用当前的参数值对训练数据进行前向传播，得到模型的预测输出。&#xA;计算损失（Compute Loss）：将模型的预测输出与真实标签进行比较，计算损失函数的值。&#xA;反向传播（Backward Propagation）：通过损失函数，计算模型参数的梯度，即损失函数对每个参数的偏导数。&#xA;参数更新（Update Parameters）：根据梯度下降或其他优化算法，沿着梯度的反方向更新参数，使损失函数减小。&#xA;我们来计算下在这个例子中参数的梯度是什么，损失函数为 $\frac{1}{n}\sum_{i=1}^{n}\left ( y_{i}-(ax_{i}+b) \right )^{2}$。 对a的偏导数为 $\frac{\partial \text{MSE}}{\partial a}$ , 这个式子可以分解为$\frac{\partial \text{MSE}}{\partial y_{pred}} \cdot \frac{\partial y_{pred}}{\partial a} $。</description>
    </item>
    <item>
      <title>About</title>
      <link>https://gufengjin770.github.io/about/</link>
      <pubDate>Mon, 26 Feb 2024 12:54:48 +0000</pubDate>
      <guid>https://gufengjin770.github.io/about/</guid>
      <description></description>
    </item>
    <item>
      <title>Blog1</title>
      <link>https://gufengjin770.github.io/post/blog1/</link>
      <pubDate>Sun, 25 Feb 2024 20:54:42 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/blog1/</guid>
      <description>hhhhhhh</description>
    </item>
  </channel>
</rss>
