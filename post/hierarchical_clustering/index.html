<!doctype html>
<html lang="en-us">
  <head>
    
    <title>Hierarchical_clustering // 金谷风的博客</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.123.3">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Gufeng Jin" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css" />
    

    
    <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Hierarchical_clustering"/>
<meta name="twitter:description" content="Hierarchical clustering（层次聚类） 是一种用于数据分析的聚类算法。它通过计算数据点之间的相似度或距离，并将最相似的数据点组合成簇，然后逐步合并这些簇，直到所有的数据点都被聚合到一个簇为止。这个过程形成了一个树状的结构，称为聚类树或者树状图。这种树状结构可以帮助我们理解数据的层次结构，并且不需要事先指定聚类的数量。
在层次聚类中，有两种主要的方法：凝聚聚类（agglomerative clustering）和分裂聚类（divisive clustering）。
凝聚聚类：该方法从每个数据点作为一个簇开始，然后逐步合并最相似的簇，直到所有的数据点都聚合到一个簇为止。 分裂聚类：与凝聚聚类相反，该方法从所有的数据点作为一个簇开始，然后逐步将最不相似的簇分裂成更小的簇，直到每个数据点都成为一个单独的簇。 层次聚类的优点之一是不需要事先指定聚类的数量，因为它可以根据数据的结构自动形成聚类。然而，它的计算复杂度通常比其他聚类算法高，特别是当数据集很大时。
凝聚聚类其步骤通常如下：
初始化：将每个数据点视为一个单独的簇。 计算相似度：计算每一对簇之间的相似度或距离。常用的距离度量包括欧氏距离、曼哈顿距离、闵可夫斯基距离等。 合并最相似的簇：找到相似度最高的两个簇，将它们合并成一个新的簇。 更新相似度矩阵：更新相似度矩阵，以反映新形成的簇与其他簇之间的相似度。 重复步骤3和4：重复步骤3和4，直到只剩下一个簇，即所有的数据点都被聚合到一个簇为止。 生成聚类树：根据合并的顺序，生成一棵树状结构，称为聚类树或者树状图。这个树状结构反映了数据点的层次结构和聚类的关系。 这段代码实现的是层次聚类中的最小距离法（Single Linkage）。在每一次迭代中，它会计算所有两两组合之间的距离，并将距离最近的两个组合并成一个新的组。然后重复这个过程，直到只剩下一个组。
import numpy as np def euclidean_distance(x1, x2): return np.sqrt(np.sum((x1 - x2)**2)) def distance_between_groups(i, j): return euclidean_distance(np.mean(cluster[f&#34;group{i}&#34;], axis=0), np.mean(cluster[f&#34;group{j}&#34;], axis=0)) data = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]) cluster = {f&#34;group{i}&#34;: [data[i].tolist()] for i in range(len(data))} while len(cluster) &gt; 1: nearest_distance = float(&#39;inf&#39;) for i in range(len(cluster) - 1): for j in range(i &#43; 1, len(cluster)): d = distance_between_groups(i, j) if d &lt; nearest_distance: nearest_distance = d ni, nj = i, j # 合并两个组的数据 group1_data = cluster[f&#34;group{ni}&#34;] group2_data = cluster[f&#34;group{nj}&#34;] merged_data = group1_data &#43; group2_data # 删除被合并的组 del cluster[f&#34;group{ni}&#34;] del cluster[f&#34;group{nj}&#34;] # 更新 cluster 字典中的键 for i, (key, value) in enumerate(cluster."/>

    <meta property="og:title" content="Hierarchical_clustering" />
<meta property="og:description" content="Hierarchical clustering（层次聚类） 是一种用于数据分析的聚类算法。它通过计算数据点之间的相似度或距离，并将最相似的数据点组合成簇，然后逐步合并这些簇，直到所有的数据点都被聚合到一个簇为止。这个过程形成了一个树状的结构，称为聚类树或者树状图。这种树状结构可以帮助我们理解数据的层次结构，并且不需要事先指定聚类的数量。
在层次聚类中，有两种主要的方法：凝聚聚类（agglomerative clustering）和分裂聚类（divisive clustering）。
凝聚聚类：该方法从每个数据点作为一个簇开始，然后逐步合并最相似的簇，直到所有的数据点都聚合到一个簇为止。 分裂聚类：与凝聚聚类相反，该方法从所有的数据点作为一个簇开始，然后逐步将最不相似的簇分裂成更小的簇，直到每个数据点都成为一个单独的簇。 层次聚类的优点之一是不需要事先指定聚类的数量，因为它可以根据数据的结构自动形成聚类。然而，它的计算复杂度通常比其他聚类算法高，特别是当数据集很大时。
凝聚聚类其步骤通常如下：
初始化：将每个数据点视为一个单独的簇。 计算相似度：计算每一对簇之间的相似度或距离。常用的距离度量包括欧氏距离、曼哈顿距离、闵可夫斯基距离等。 合并最相似的簇：找到相似度最高的两个簇，将它们合并成一个新的簇。 更新相似度矩阵：更新相似度矩阵，以反映新形成的簇与其他簇之间的相似度。 重复步骤3和4：重复步骤3和4，直到只剩下一个簇，即所有的数据点都被聚合到一个簇为止。 生成聚类树：根据合并的顺序，生成一棵树状结构，称为聚类树或者树状图。这个树状结构反映了数据点的层次结构和聚类的关系。 这段代码实现的是层次聚类中的最小距离法（Single Linkage）。在每一次迭代中，它会计算所有两两组合之间的距离，并将距离最近的两个组合并成一个新的组。然后重复这个过程，直到只剩下一个组。
import numpy as np def euclidean_distance(x1, x2): return np.sqrt(np.sum((x1 - x2)**2)) def distance_between_groups(i, j): return euclidean_distance(np.mean(cluster[f&#34;group{i}&#34;], axis=0), np.mean(cluster[f&#34;group{j}&#34;], axis=0)) data = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]) cluster = {f&#34;group{i}&#34;: [data[i].tolist()] for i in range(len(data))} while len(cluster) &gt; 1: nearest_distance = float(&#39;inf&#39;) for i in range(len(cluster) - 1): for j in range(i &#43; 1, len(cluster)): d = distance_between_groups(i, j) if d &lt; nearest_distance: nearest_distance = d ni, nj = i, j # 合并两个组的数据 group1_data = cluster[f&#34;group{ni}&#34;] group2_data = cluster[f&#34;group{nj}&#34;] merged_data = group1_data &#43; group2_data # 删除被合并的组 del cluster[f&#34;group{ni}&#34;] del cluster[f&#34;group{nj}&#34;] # 更新 cluster 字典中的键 for i, (key, value) in enumerate(cluster." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gufengjin770.github.io/post/hierarchical_clustering/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-03-09T13:24:55+00:00" />
<meta property="article:modified_time" content="2024-03-09T13:24:55+00:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://gufengjin770.github.io/"><img class="app-header-avatar" src="/image1.jpg" alt="Gufeng Jin" /></a>
      <span class="app-header-title">金谷风的博客</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>兢兢以强</p>
      <div class="app-header-social">
        
          <a href="https://github.com/gufengjin770" target="_blank" rel="noreferrer noopener me">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="mailto:gufengjin770@gmail.com" target="_blank" rel="noreferrer noopener me">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-mail">
  <title>E-mail</title>
  <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Hierarchical_clustering</h1>
      <div class="post-meta">
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Mar 9, 2024
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://gufengjin770.github.io/tags/%E7%AE%97%E6%B3%95algorithm/">算法algorithm</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h4 id="hierarchical-clustering层次聚类">Hierarchical clustering（层次聚类）</h4>
<p>是一种用于数据分析的聚类算法。它通过计算数据点之间的相似度或距离，并将最相似的数据点组合成簇，然后逐步合并这些簇，直到所有的数据点都被聚合到一个簇为止。这个过程形成了一个树状的结构，称为聚类树或者树状图。这种树状结构可以帮助我们理解数据的层次结构，并且不需要事先指定聚类的数量。</p>
<p>在层次聚类中，有两种主要的方法：凝聚聚类（agglomerative clustering）和分裂聚类（divisive clustering）。</p>
<ul>
<li>凝聚聚类：该方法从每个数据点作为一个簇开始，然后逐步合并最相似的簇，直到所有的数据点都聚合到一个簇为止。</li>
<li>分裂聚类：与凝聚聚类相反，该方法从所有的数据点作为一个簇开始，然后逐步将最不相似的簇分裂成更小的簇，直到每个数据点都成为一个单独的簇。</li>
</ul>
<p>层次聚类的优点之一是不需要事先指定聚类的数量，因为它可以根据数据的结构自动形成聚类。然而，它的计算复杂度通常比其他聚类算法高，特别是当数据集很大时。</p>
<p>凝聚聚类其步骤通常如下：</p>
<ol>
<li>初始化：将每个数据点视为一个单独的簇。</li>
<li>计算相似度：计算每一对簇之间的相似度或距离。常用的距离度量包括欧氏距离、曼哈顿距离、闵可夫斯基距离等。</li>
<li>合并最相似的簇：找到相似度最高的两个簇，将它们合并成一个新的簇。</li>
<li>更新相似度矩阵：更新相似度矩阵，以反映新形成的簇与其他簇之间的相似度。</li>
<li>重复步骤3和4：重复步骤3和4，直到只剩下一个簇，即所有的数据点都被聚合到一个簇为止。</li>
<li>生成聚类树：根据合并的顺序，生成一棵树状结构，称为聚类树或者树状图。这个树状结构反映了数据点的层次结构和聚类的关系。</li>
</ol>
<p>这段代码实现的是层次聚类中的最小距离法（Single Linkage）。在每一次迭代中，它会计算所有两两组合之间的距离，并将距离最近的两个组合并成一个新的组。然后重复这个过程，直到只剩下一个组。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">euclidean_distance</span>(x1, x2):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>sqrt(np<span style="color:#f92672">.</span>sum((x1 <span style="color:#f92672">-</span> x2)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">distance_between_groups</span>(i, j):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> euclidean_distance(np<span style="color:#f92672">.</span>mean(cluster[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>), np<span style="color:#f92672">.</span>mean(cluster[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>j<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">7</span>], [<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span>], [<span style="color:#ae81ff">25</span>, <span style="color:#ae81ff">80</span>]])
</span></span><span style="display:flex;"><span>cluster <span style="color:#f92672">=</span> {<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>: [data[i]<span style="color:#f92672">.</span>tolist()] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(data))}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">while</span> len(cluster) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>    nearest_distance <span style="color:#f92672">=</span> float(<span style="color:#e6db74">&#39;inf&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(cluster) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, len(cluster)):
</span></span><span style="display:flex;"><span>            d <span style="color:#f92672">=</span> distance_between_groups(i, j)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> d <span style="color:#f92672">&lt;</span> nearest_distance:
</span></span><span style="display:flex;"><span>                nearest_distance <span style="color:#f92672">=</span> d
</span></span><span style="display:flex;"><span>                ni, nj <span style="color:#f92672">=</span> i, j
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 合并两个组的数据</span>
</span></span><span style="display:flex;"><span>    group1_data <span style="color:#f92672">=</span> cluster[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>ni<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>]
</span></span><span style="display:flex;"><span>    group2_data <span style="color:#f92672">=</span> cluster[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>nj<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>]
</span></span><span style="display:flex;"><span>    merged_data <span style="color:#f92672">=</span> group1_data <span style="color:#f92672">+</span> group2_data
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 删除被合并的组</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">del</span> cluster[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>ni<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">del</span> cluster[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>nj<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 更新 cluster 字典中的键</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i, (key, value) <span style="color:#f92672">in</span> enumerate(cluster<span style="color:#f92672">.</span>copy()<span style="color:#f92672">.</span>items()):
</span></span><span style="display:flex;"><span>        cluster[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>] <span style="color:#f92672">=</span> cluster<span style="color:#f92672">.</span>pop(key)
</span></span><span style="display:flex;"><span>    cluster[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;group</span><span style="color:#e6db74">{</span>i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>] <span style="color:#f92672">=</span> merged_data
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(cluster)
</span></span></code></pre></div><p>生成聚类树</p>
<p><img src="../../image/tree_plot.png" alt="tree plot"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.cluster.hierarchy <span style="color:#f92672">import</span> dendrogram, linkage
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">7</span>], [<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span>], [<span style="color:#ae81ff">25</span>, <span style="color:#ae81ff">80</span>]])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用ward方法进行层次聚类</span>
</span></span><span style="display:flex;"><span>Z <span style="color:#f92672">=</span> linkage(data, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;single&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制聚类树</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>dendrogram(Z)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Hierarchical Clustering Dendrogram&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Sample Index&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Distance&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div>
    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
