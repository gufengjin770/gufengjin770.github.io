<!doctype html>
<html lang="en-us">
  <head>
    
    <title>Kmeans // 金谷风的博客</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.123.3">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Gufeng Jin" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.5b1fcc8902588589c4767187402a3c29f8b8d7a6fdef6d9f8f77045bb0d14fee.css" />
    

    
    <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Kmeans"/>
<meta name="twitter:description" content="kmeans&#43;&#43;算法 K均值&#43;&#43;（K-means&#43;&#43;）是一种改进的K均值聚类算法，旨在解决传统K均值算法在初始质心选择上的随机性问题，提高了算法的收敛速度和聚类质量。
K均值算法是一种常用的无监督聚类方法，其基本思想是将数据集划分为K个簇，使得同一簇内的数据点彼此相似，而不同簇之间的数据点尽可能不相似。该算法的主要步骤包括选择初始质心、分配数据点到最近的质心所属的簇、更新质心位置等。
K均值&#43;&#43;算法通过改进初始质心的选择，来提高K均值算法的效果。其主要思想是在选择初始质心时，首先选择一个质心作为第一个簇的中心，然后按照一定的概率分布选择下一个质心，使得离已选择的质心越远的点被选作下一个质心的概率越大，以此类推，直到选择完所有的初始质心。这样做可以避免初始质心选择过于集中或者过于分散的情况，提高了算法的鲁棒性和收敛速度。
K均值&#43;&#43;算法的步骤如下：
从数据集中随机选择一个点作为第一个质心。 对于数据集中的每个点，计算其到已选择的质心的距离，并选择一个距离当前已选择质心最远的点作为下一个质心，选择的概率与距离的平方成正比。 重复步骤2，直到选择完所有的质心。 使用K均值算法进行迭代优化，直到质心不再发生变化或者达到最大迭代次数。 K均值&#43;&#43;算法相比传统K均值算法，在相同迭代次数下能够得到更好的聚类效果，并且对初始质心的选择更加稳定和可靠。因此，在实际应用中，K均值&#43;&#43;算法更为常用。
import numpy as np # 使用库函数计算欧氏距离 def euclidean(point1, point2): return np.linalg.norm(point1 - point2) # 使用K均值&#43;&#43;算法选择初始质心 def initialize_centroids(data, num_clusters): centroids = np.zeros((num_clusters, data.shape[1])) centroids[0] = data[np.random.randint(0, len(data))] for i in range(1, num_clusters): distances = np.array([min([euclidean(point, centroid) for centroid in centroids[:i]]) for point in data]) probabilities = distances / distances.sum() centroids[i] = data[np.random.choice(len(data), p=probabilities)] return centroids # K均值算法 def kmeans(data, num_clusters, max_iter=100, tol=1e-4): centroids = initialize_centroids(data, num_clusters) for _ in range(max_iter): # 分配数据点到最近的质心 clusters = {i: [] for i in range(num_clusters)} for point in data: distances = [euclidean(point, centroid) for centroid in centroids] cluster = np."/>

    <meta property="og:title" content="Kmeans" />
<meta property="og:description" content="kmeans&#43;&#43;算法 K均值&#43;&#43;（K-means&#43;&#43;）是一种改进的K均值聚类算法，旨在解决传统K均值算法在初始质心选择上的随机性问题，提高了算法的收敛速度和聚类质量。
K均值算法是一种常用的无监督聚类方法，其基本思想是将数据集划分为K个簇，使得同一簇内的数据点彼此相似，而不同簇之间的数据点尽可能不相似。该算法的主要步骤包括选择初始质心、分配数据点到最近的质心所属的簇、更新质心位置等。
K均值&#43;&#43;算法通过改进初始质心的选择，来提高K均值算法的效果。其主要思想是在选择初始质心时，首先选择一个质心作为第一个簇的中心，然后按照一定的概率分布选择下一个质心，使得离已选择的质心越远的点被选作下一个质心的概率越大，以此类推，直到选择完所有的初始质心。这样做可以避免初始质心选择过于集中或者过于分散的情况，提高了算法的鲁棒性和收敛速度。
K均值&#43;&#43;算法的步骤如下：
从数据集中随机选择一个点作为第一个质心。 对于数据集中的每个点，计算其到已选择的质心的距离，并选择一个距离当前已选择质心最远的点作为下一个质心，选择的概率与距离的平方成正比。 重复步骤2，直到选择完所有的质心。 使用K均值算法进行迭代优化，直到质心不再发生变化或者达到最大迭代次数。 K均值&#43;&#43;算法相比传统K均值算法，在相同迭代次数下能够得到更好的聚类效果，并且对初始质心的选择更加稳定和可靠。因此，在实际应用中，K均值&#43;&#43;算法更为常用。
import numpy as np # 使用库函数计算欧氏距离 def euclidean(point1, point2): return np.linalg.norm(point1 - point2) # 使用K均值&#43;&#43;算法选择初始质心 def initialize_centroids(data, num_clusters): centroids = np.zeros((num_clusters, data.shape[1])) centroids[0] = data[np.random.randint(0, len(data))] for i in range(1, num_clusters): distances = np.array([min([euclidean(point, centroid) for centroid in centroids[:i]]) for point in data]) probabilities = distances / distances.sum() centroids[i] = data[np.random.choice(len(data), p=probabilities)] return centroids # K均值算法 def kmeans(data, num_clusters, max_iter=100, tol=1e-4): centroids = initialize_centroids(data, num_clusters) for _ in range(max_iter): # 分配数据点到最近的质心 clusters = {i: [] for i in range(num_clusters)} for point in data: distances = [euclidean(point, centroid) for centroid in centroids] cluster = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gufengjin770.github.io/post/kmeans/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-03-06T21:20:34+00:00" />
<meta property="article:modified_time" content="2024-03-06T21:20:34+00:00" />



  </head>
  <body>
    <header class="app-header">
      <a href="https://gufengjin770.github.io/"><img class="app-header-avatar" src="/image1.jpg" alt="Gufeng Jin" /></a>
      <span class="app-header-title">金谷风的博客</span>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             - 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
             - 
          
          <a class="app-header-menu-item" href="/about/">About</a>
      </nav>
      <p>兢兢以强</p>
      <div class="app-header-social">
        
          <a href="https://github.com/gufengjin770" target="_blank" rel="noreferrer noopener me">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>Github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg>
          </a>
        
          <a href="mailto:gufengjin770@gmail.com" target="_blank" rel="noreferrer noopener me">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-mail">
  <title>E-mail</title>
  <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline>
</svg>
          </a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Kmeans</h1>
      <div class="post-meta">
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Mar 6, 2024
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
        <div>
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
              <a class="tag" href="https://gufengjin770.github.io/tags/%E7%AE%97%E6%B3%95algorithm/">算法algorithm</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h3 id="kmeans算法">kmeans++算法</h3>
<p>K均值++（K-means++）是一种改进的K均值聚类算法，旨在解决传统K均值算法在初始质心选择上的随机性问题，提高了算法的收敛速度和聚类质量。</p>
<p>K均值算法是一种常用的无监督聚类方法，其基本思想是将数据集划分为K个簇，使得同一簇内的数据点彼此相似，而不同簇之间的数据点尽可能不相似。该算法的主要步骤包括选择初始质心、分配数据点到最近的质心所属的簇、更新质心位置等。</p>
<p>K均值++算法通过改进初始质心的选择，来提高K均值算法的效果。其主要思想是在选择初始质心时，首先选择一个质心作为第一个簇的中心，然后按照一定的概率分布选择下一个质心，使得离已选择的质心越远的点被选作下一个质心的概率越大，以此类推，直到选择完所有的初始质心。这样做可以避免初始质心选择过于集中或者过于分散的情况，提高了算法的鲁棒性和收敛速度。</p>
<p>K均值++算法的步骤如下：</p>
<ol>
<li>从数据集中随机选择一个点作为第一个质心。</li>
<li>对于数据集中的每个点，计算其到已选择的质心的距离，并选择一个距离当前已选择质心最远的点作为下一个质心，选择的概率与距离的平方成正比。</li>
<li>重复步骤2，直到选择完所有的质心。</li>
<li>使用K均值算法进行迭代优化，直到质心不再发生变化或者达到最大迭代次数。</li>
</ol>
<p>K均值++算法相比传统K均值算法，在相同迭代次数下能够得到更好的聚类效果，并且对初始质心的选择更加稳定和可靠。因此，在实际应用中，K均值++算法更为常用。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用库函数计算欧氏距离</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">euclidean</span>(point1, point2):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>norm(point1 <span style="color:#f92672">-</span> point2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用K均值++算法选择初始质心</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initialize_centroids</span>(data, num_clusters):
</span></span><span style="display:flex;"><span>    centroids <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((num_clusters, data<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>    centroids[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> data[np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, len(data))]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, num_clusters):
</span></span><span style="display:flex;"><span>        distances <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([min([euclidean(point, centroid) <span style="color:#66d9ef">for</span> centroid <span style="color:#f92672">in</span> centroids[:i]]) <span style="color:#66d9ef">for</span> point <span style="color:#f92672">in</span> data])
</span></span><span style="display:flex;"><span>        probabilities <span style="color:#f92672">=</span> distances <span style="color:#f92672">/</span> distances<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>        centroids[i] <span style="color:#f92672">=</span> data[np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(len(data), p<span style="color:#f92672">=</span>probabilities)]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> centroids
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># K均值算法</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kmeans</span>(data, num_clusters, max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, tol<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-4</span>):
</span></span><span style="display:flex;"><span>    centroids <span style="color:#f92672">=</span> initialize_centroids(data, num_clusters)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(max_iter):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 分配数据点到最近的质心</span>
</span></span><span style="display:flex;"><span>        clusters <span style="color:#f92672">=</span> {i: [] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_clusters)}
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> point <span style="color:#f92672">in</span> data:
</span></span><span style="display:flex;"><span>            distances <span style="color:#f92672">=</span> [euclidean(point, centroid) <span style="color:#66d9ef">for</span> centroid <span style="color:#f92672">in</span> centroids]
</span></span><span style="display:flex;"><span>            cluster <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmin(distances)
</span></span><span style="display:flex;"><span>            clusters[cluster]<span style="color:#f92672">.</span>append(point)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 更新质心</span>
</span></span><span style="display:flex;"><span>        new_centroids <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([np<span style="color:#f92672">.</span>mean(clusters[i], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_clusters)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 判断是否收敛</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>allclose(centroids, new_centroids, atol<span style="color:#f92672">=</span>tol):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        centroids <span style="color:#f92672">=</span> new_centroids
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> clusters
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 示例数据</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span> , <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>], [<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">6</span>],[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>], [<span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">12</span>]])
</span></span><span style="display:flex;"><span>num_clusters <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>max_iter <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>clusters <span style="color:#f92672">=</span> kmeans(data, num_clusters, max_iter)
</span></span><span style="display:flex;"><span>print(clusters)
</span></span></code></pre></div>
    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
