<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 金谷风的博客</title>
    <link>https://gufengjin770.github.io/post/</link>
    <description>Recent content in Posts on 金谷风的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Apr 2024 00:11:49 +0100</lastBuildDate>
    <atom:link href="https://gufengjin770.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CNN-MINST</title>
      <link>https://gufengjin770.github.io/post/cnn-minst/</link>
      <pubDate>Sat, 06 Apr 2024 00:11:49 +0100</pubDate>
      <guid>https://gufengjin770.github.io/post/cnn-minst/</guid>
      <description># This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&amp;#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &amp;#34;../input/&amp;#34; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.</description>
    </item>
    <item>
      <title>Seq to Seq</title>
      <link>https://gufengjin770.github.io/post/seq-to-seq/</link>
      <pubDate>Fri, 05 Apr 2024 23:58:23 +0100</pubDate>
      <guid>https://gufengjin770.github.io/post/seq-to-seq/</guid>
      <description>import pandas as pd import re import numpy as np import spacy import datasets import torchtext import tqdm import evaluate from datasets import Dataset, DatasetDict import random from typing import Tuple import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch import Tensor c:\Users\gufen\AppData\Local\Programs\Python\Python311\Lib\site-packages\tqdm\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm file_path = &amp;#39;G:\\ted2020.tsv&amp;#39; column_data = pd.</description>
    </item>
    <item>
      <title>Hierarchical_clustering</title>
      <link>https://gufengjin770.github.io/post/hierarchical_clustering/</link>
      <pubDate>Sat, 09 Mar 2024 13:24:55 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/hierarchical_clustering/</guid>
      <description>Hierarchical clustering（层次聚类） 是一种用于数据分析的聚类算法。它通过计算数据点之间的相似度或距离，并将最相似的数据点组合成簇，然后逐步合并这些簇，直到所有的数据点都被聚合到一个簇为止。这个过程形成了一个树状的结构，称为聚类树或者树状图。这种树状结构可以帮助我们理解数据的层次结构，并且不需要事先指定聚类的数量。&#xA;在层次聚类中，有两种主要的方法：凝聚聚类（agglomerative clustering）和分裂聚类（divisive clustering）。&#xA;凝聚聚类：该方法从每个数据点作为一个簇开始，然后逐步合并最相似的簇，直到所有的数据点都聚合到一个簇为止。 分裂聚类：与凝聚聚类相反，该方法从所有的数据点作为一个簇开始，然后逐步将最不相似的簇分裂成更小的簇，直到每个数据点都成为一个单独的簇。 层次聚类的优点之一是不需要事先指定聚类的数量，因为它可以根据数据的结构自动形成聚类。然而，它的计算复杂度通常比其他聚类算法高，特别是当数据集很大时。&#xA;凝聚聚类其步骤通常如下：&#xA;初始化：将每个数据点视为一个单独的簇。 计算相似度：计算每一对簇之间的相似度或距离。常用的距离度量包括欧氏距离、曼哈顿距离、闵可夫斯基距离等。 合并最相似的簇：找到相似度最高的两个簇，将它们合并成一个新的簇。 更新相似度矩阵：更新相似度矩阵，以反映新形成的簇与其他簇之间的相似度。 重复步骤3和4：重复步骤3和4，直到只剩下一个簇，即所有的数据点都被聚合到一个簇为止。 生成聚类树：根据合并的顺序，生成一棵树状结构，称为聚类树或者树状图。这个树状结构反映了数据点的层次结构和聚类的关系。 这段代码实现的是层次聚类中的最小距离法（Single Linkage）。在每一次迭代中，它会计算所有两两组合之间的距离，并将距离最近的两个组合并成一个新的组。然后重复这个过程，直到只剩下一个组。&#xA;import numpy as np def euclidean_distance(x1, x2): return np.sqrt(np.sum((x1 - x2)**2)) def distance_between_groups(i, j): return euclidean_distance(np.mean(cluster[f&amp;#34;group{i}&amp;#34;], axis=0), np.mean(cluster[f&amp;#34;group{j}&amp;#34;], axis=0)) data = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]]) cluster = {f&amp;#34;group{i}&amp;#34;: [data[i].tolist()] for i in range(len(data))} while len(cluster) &amp;gt; 1: nearest_distance = float(&amp;#39;inf&amp;#39;) for i in range(len(cluster) - 1): for j in range(i + 1, len(cluster)): d = distance_between_groups(i, j) if d &amp;lt; nearest_distance: nearest_distance = d ni, nj = i, j # 合并两个组的数据 group1_data = cluster[f&amp;#34;group{ni}&amp;#34;] group2_data = cluster[f&amp;#34;group{nj}&amp;#34;] merged_data = group1_data + group2_data # 删除被合并的组 del cluster[f&amp;#34;group{ni}&amp;#34;] del cluster[f&amp;#34;group{nj}&amp;#34;] # 更新 cluster 字典中的键 for i, (key, value) in enumerate(cluster.</description>
    </item>
    <item>
      <title>K Medoids</title>
      <link>https://gufengjin770.github.io/post/k-medoids/</link>
      <pubDate>Sat, 09 Mar 2024 13:23:56 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/k-medoids/</guid>
      <description>K-Medoids 是一种聚类算法，它是K-Means的一种变体。在K-Medoids算法中，每个簇的代表不再是簇内所有点的平均值（即簇的中心），而是选择簇内最能代表该簇的点，这些点被称为medoids。与K-Means相比，K-Medoids更有效地处理数据中的异常值和噪声。&#xA;import numpy as np # 使用库函数计算欧氏距离 def euclidean(point1, point2): return np.linalg.norm(point1 - point2) # 使用K-Medoids算法选择初始medoids def initialize_medoids(data, num_clusters): medoids = np.zeros((num_clusters, data.shape[1])) medoids[0] = data[np.random.randint(0, len(data))] for i in range(1, num_clusters): distances = np.array([min([euclidean(point, medoid) for medoid in medoids[:i]]) for point in data]) probabilities = distances / distances.sum() medoids[i] = data[np.random.choice(len(data), p=probabilities)] return medoids # K-Medoids算法 def kmedoids(data, num_clusters, max_iter=100, tol=1e-4): medoids = initialize_medoids(data, num_clusters) for _ in range(max_iter): # 分配数据点到最近的medoid clusters = {i: [] for i in range(num_clusters)} for point in data: distances = [euclidean(point, medoid) for medoid in medoids] cluster = np.</description>
    </item>
    <item>
      <title>DBSCAN</title>
      <link>https://gufengjin770.github.io/post/dbscan/</link>
      <pubDate>Sat, 09 Mar 2024 13:22:41 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/dbscan/</guid>
      <description>DBSCAN（Density-Based Spatial Clustering of Applications with Noise） 是一种基于密度的聚类算法，能够识别任意形状的簇，并能够有效地处理噪声数据。DBSCAN通过定义一定的密度阈值来识别簇，而不需要事先指定簇的数量。&#xA;DBSCAN的主要思想是根据数据点的密度来确定簇的边界。它的工作原理如下：&#xA;核心点（Core Points）：对于给定的密度参数ε（epsilon）和最小邻域内的点数目minPts，如果一个点的ε-邻域（包括该点在内的半径为ε的区域）内至少包含minPts个数据点，则这个点被称为核心点。&#xA;直接密度可达（Directly Density-Reachable）：如果一个点在另一个点的ε-邻域内，并且这个点是核心点，则这个点被认为是直接密度可达的。&#xA;密度可达（Density-Reachable）：如果存在一系列核心点，使得从第一个核心点到最后一个核心点经过的每个核心点都是直接密度可达的，则最后一个核心点是从第一个核心点密度可达的。&#xA;密度相连（Density-Connected）：如果存在一个核心点C，使得点A和点B都是从核心点C密度可达的，则点A和点B被认为是密度相连的。&#xA;基于以上定义，DBSCAN算法将数据点分为三种类型：核心点、边界点和噪声点。核心点属于一个簇，边界点位于某个簇的边界上，而噪声点不属于任何簇。&#xA;DBSCAN的优点包括对噪声和离群点的鲁棒性、能够发现任意形状的簇、不需要事先指定簇的数量等。但是，DBSCAN的性能受到密度参数ε和最小邻域内的点数目minPts的选择影响较大，需要调参来获得最佳结果。&#xA;import numpy as np def euclidean_distance(x1, x2): return np.sqrt(np.sum((x1 - x2)**2)) def range_query(data, point_index, eps): neighbors = [] for i in range(len(data)): if euclidean_distance(data[point_index], data[i]) &amp;lt;= eps: neighbors.append(i) return neighbors def dbscan(data, eps, min_samples): labels = np.full(len(data), -1) # -1表示噪声点 cluster_id = 0 for i in range(len(data)): if labels[i] != -1: continue neighbors = range_query(data, i, eps) if len(neighbors) &amp;lt; min_samples: labels[i] = -1 # 标记为噪声点 else: cluster_id += 1 labels[i] = cluster_id expand_cluster(data, labels, i, neighbors, cluster_id, eps, min_samples) cluster = { f&amp;#34;group{i}&amp;#34; : [] for i in set(labels)} for i in range(len(data)): cluster[f&amp;#34;group{labels[i]}&amp;#34;].</description>
    </item>
    <item>
      <title>Kmeans</title>
      <link>https://gufengjin770.github.io/post/kmeans/</link>
      <pubDate>Wed, 06 Mar 2024 21:20:34 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/kmeans/</guid>
      <description>kmeans++算法 K均值++（K-means++）是一种改进的K均值聚类算法，旨在解决传统K均值算法在初始质心选择上的随机性问题，提高了算法的收敛速度和聚类质量。&#xA;K均值算法是一种常用的无监督聚类方法，其基本思想是将数据集划分为K个簇，使得同一簇内的数据点彼此相似，而不同簇之间的数据点尽可能不相似。该算法的主要步骤包括选择初始质心、分配数据点到最近的质心所属的簇、更新质心位置等。&#xA;K均值++算法通过改进初始质心的选择，来提高K均值算法的效果。其主要思想是在选择初始质心时，首先选择一个质心作为第一个簇的中心，然后按照一定的概率分布选择下一个质心，使得离已选择的质心越远的点被选作下一个质心的概率越大，以此类推，直到选择完所有的初始质心。这样做可以避免初始质心选择过于集中或者过于分散的情况，提高了算法的鲁棒性和收敛速度。&#xA;K均值++算法的步骤如下：&#xA;从数据集中随机选择一个点作为第一个质心。 对于数据集中的每个点，计算其到已选择的质心的距离，并选择一个距离当前已选择质心最远的点作为下一个质心，选择的概率与距离的平方成正比。 重复步骤2，直到选择完所有的质心。 使用K均值算法进行迭代优化，直到质心不再发生变化或者达到最大迭代次数。 K均值++算法相比传统K均值算法，在相同迭代次数下能够得到更好的聚类效果，并且对初始质心的选择更加稳定和可靠。因此，在实际应用中，K均值++算法更为常用。&#xA;import numpy as np # 使用库函数计算欧氏距离 def euclidean(point1, point2): return np.linalg.norm(point1 - point2) # 使用K均值++算法选择初始质心 def initialize_centroids(data, num_clusters): centroids = np.zeros((num_clusters, data.shape[1])) centroids[0] = data[np.random.randint(0, len(data))] for i in range(1, num_clusters): distances = np.array([min([euclidean(point, centroid) for centroid in centroids[:i]]) for point in data]) probabilities = distances / distances.sum() centroids[i] = data[np.random.choice(len(data), p=probabilities)] return centroids # K均值算法 def kmeans(data, num_clusters, max_iter=100, tol=1e-4): centroids = initialize_centroids(data, num_clusters) for _ in range(max_iter): # 分配数据点到最近的质心 clusters = {i: [] for i in range(num_clusters)} for point in data: distances = [euclidean(point, centroid) for centroid in centroids] cluster = np.</description>
    </item>
    <item>
      <title>不依靠PyTorch和TensorFlow做手写数字识别</title>
      <link>https://gufengjin770.github.io/post/deeplearning2/</link>
      <pubDate>Mon, 04 Mar 2024 04:53:54 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/deeplearning2/</guid>
      <description>不依靠PyTorch和TensorFlow做手写数字识别 这章我们来看下如何不通过调用python深度学习的库来写CNN网络。&#xA;所有机器学习都可以看成三个步骤：&#xA;定义方程的框架，输入是什么 输出是什么 中间如何从输入到输出&#xA;定义损失函数 衡量模型预测与实际目标之间差异的指标。&#xA;最优化算法 根据定义的框架和损失函数找出模型最优的参数&#xA;在下列例子中我们用到的是MNIST 数据集，一个包含手写数字图像的经典数据集，用于机器学习和模式识别的基准测试。这些图像是 28x28 像素的灰度图像，表示手写的数字 0 到 9。在代码中，x_train 包含了 60000 个训练样本，每个样本是一个经过归一化处理的大小为 784 (28x28) 的一维数组，代表一个手写数字图像。而 t_train 则是每个样本对应的标签，使用了 one-hot 编码，表示了对应的数字类别。&#xA;下载数据集&#xA;import sys, os sys.path.append(os.pardir) import numpy as np from dataset.mnist import load_mnist from collections import OrderedDict (x_train, t_train), (x_test, t_test) = \ load_mnist(normalize=True, one_hot_label=True) print(x_train.shape) # (60000, 784) print(t_train.shape) # (60000, 10) 打印下图片&#xA;import matplotlib.pyplot as plt # 打印前两个样本及其标签 num_samples_to_display = 2 for i in range(num_samples_to_display): sample_image = x_train[i].</description>
    </item>
    <item>
      <title>如何用梯度下降法训练一个简单的线性模型</title>
      <link>https://gufengjin770.github.io/post/deeplearning1/</link>
      <pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://gufengjin770.github.io/post/deeplearning1/</guid>
      <description>如何用梯度下降法训练一个简单的线性模型 数学模型是什么？ 数学模型能够从数据中学习并进行预测或决策。这些模型可以用于解决各种问题，包括分类 Classification, 回归 Regression等。模型通常被用来发现数据中的模式和关联，从而进行预测或做出决策。&#xA;模型通常包含输入值和输出值。输入值 Input通常是一组特征或属性，用来描述数据的各个方面。输出值 Output则是模型根据输入值进行预测或分类的结果。&#xA;在监督学习 Supervised learning中，模型通过学习输入值和对应的输出值之间的关系来进行预测或分类。这些输入值和输出值通常被组织成训练集 Training set和验证集 Validation set，模型通过训练集学习输入和输出之间的映射关系。然后使用验证集来评估模型的性能，以便更好地了解模型在未见过的数据上的表现。通过与训练集分开的独立数据进行评估，可以更客观地判断模型的泛化能力，即模型对未知数据的适应能力。一旦训练完成，模型就可以用来对新的输入值进行预测或分类。&#xA;在 无监督学习 Unsupervised learning中，模型不需要标记的输出值，而是试图发现数据中的结构或模式。输入值被用来描述数据，而模型则尝试根据这些输入值发现数据的内在结构或者进行聚类等任务。&#xA;如何用梯度下降法 Gradient Descent Method训练一个简单的线性模型？ 先来看下如何训练监督学习类问题，我们将用梯度下降法训练一个简单的线性模型。&#xA;首先，我们需要准备包含输入值和对应输出值（标签）的数据集。在下面代码中例子里输入值是 $x = [1, 2, 3, 4]$，输出值是 $y = [2, 3, 4, 5]$。&#xA;第二步选择模型，在这个例子中我们选择线性模型， $y_{pred} = ax + b$, 其中x是输入值，$y_{pred}$是预测值，而a和b是模型的两个参数。&#xA;第三步定义损失函数，我们的目标是模型能根据输入的$x$准确的输出$y$的值，所以我们需要预测值尽可能的等于输出值，也就是$y-y_{pred}$ 尽可能接近零。我们的损失函数是均方误差（MSE），定义如下： $$ \frac{1}{n}\sum_{i=1}^{n}\left ( y_{i}-(ax_{i}+b) \right )^{2} $$ 其中，$n$ 是样本数量，$y_i$ 是实际标签，$x_i$ 是对应的特征值。我们的目标就是通过优化算法修改参数 (a,b) 让损失函数尽可能小。&#xA;第四部选择优化算法，我们选择梯度下降法训练。梯度下降法包含四个步骤，&#xA;前向传播（Forward Propagation）：使用当前的参数值对训练数据进行前向传播，得到模型的预测输出。&#xA;计算损失（Compute Loss）：将模型的预测输出与真实标签进行比较，计算损失函数的值。&#xA;反向传播（Backward Propagation）：通过损失函数，计算模型参数的梯度，即损失函数对每个参数的偏导数。&#xA;参数更新（Update Parameters）：根据梯度下降或其他优化算法，沿着梯度的反方向更新参数，使损失函数减小。&#xA;我们来计算下在这个例子中参数的梯度是什么，损失函数为 $\frac{1}{n}\sum_{i=1}^{n}\left ( y_{i}-(ax_{i}+b) \right )^{2}$。 对a的偏导数为 $\frac{\partial \text{MSE}}{\partial a}$ , 这个式子可以分解为$\frac{\partial \text{MSE}}{\partial y_{pred}} \cdot \frac{\partial y_{pred}}{\partial a} $。</description>
    </item>
  </channel>
</rss>
